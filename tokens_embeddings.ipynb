{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da344a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "device_map=\"cuda\",\n",
    "torch_dtype=\"auto\",\n",
    "trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f2a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap.Explain how it happened.<|assistant|>\"\n",
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "# Generate the text\n",
    "generation_output = model.generate(\n",
    "input_ids=input_ids,\n",
    "max_new_tokens=20\n",
    ")\n",
    "# Print the output\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8de93bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the ids to see the tokens back \n",
    "for id in input_ids[0]:\n",
    "    print(tokenizer.decode(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c854f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print generation_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9747b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(3323))\n",
    "print(tokenizer.decode(622))\n",
    "print(tokenizer.decode([3323, 622]))\n",
    "print(tokenizer.decode(29901))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6f9bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_list = [\n",
    "'102;194;165', '252;141;98', '141;160;203',\n",
    "'231;138;195', '166;216;84', '255;217;47'\n",
    "]\n",
    "def show_tokens(sentence, tokenizer_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    token_ids = tokenizer(sentence).input_ids\n",
    "    for idx, t in enumerate(token_ids):\n",
    "    print( f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' + tokenizer.decode(t) +'\\x1b[0m',end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a5ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate contextualized word embeddings \n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "# Load a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "# Load a language model\n",
    "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
    "# Tokenize the sentence\n",
    "tokens = tokenizer('Hello world', return_tensors='pt')\n",
    "# Process the tokens\n",
    "output = model(**tokens)[0]\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e627a22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in tokens['input_ids'][0]:\n",
    "    print(tokenizer.decode(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58952a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text embeddings \n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Load model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "# Convert text to text embeddings\n",
    "vector = model.encode(\"Best movie ever!\")\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08426a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pretrained word embeddings\n",
    "import gensim.downloader as api\n",
    "# Download embeddings (66MB, glove, trained on wikipedia, vector size: 50)\n",
    "# Other options include \"word2vec-google-news-300\"\n",
    "# More options at https://github.com/RaRe-Technologies/gensim-data\n",
    "model = api.load(\"glove-wiki-gigaword-50\")\n",
    "model.most_similar([model['king']], topn=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154b2384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Song Recommendation embedding model\n",
    "import pandas as pd\n",
    "from urllib import request\n",
    "# Get the playlist dataset file\n",
    "data = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/train.txt')\n",
    "# Parse the playlist dataset file. Skip the first two lines as\n",
    "# they only contain metadata\n",
    "lines = data.read().decode(\"utf-8\").split('\\n')[2:]\n",
    "# Remove playlists with only one song\n",
    "playlists = [s.rstrip().split() for s in lines if len(s.split()) > 1]\n",
    "# Load song metadata\n",
    "songs_file = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/song_hash.txt')\n",
    "songs_file = songs_file.read().decode(\"utf-8\").split('\\n')\n",
    "songs = [s.rstrip().split('\\t') for s in songs_file]\n",
    "songs_df = pd.DataFrame(data=songs, columns = ['id', 'title', 'artist'])\n",
    "songs_df = songs_df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1ddafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets inspect the playlists\n",
    "print( 'Playlist #1:\\n ', playlists[0], '\\n')\n",
    "print( 'Playlist #2:\\n ', playlists[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70812319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets train the model\n",
    "from gensim.models import Word2Vec\n",
    "# Train our Word2Vec model\n",
    "model = Word2Vec(playlists, vector_size=32, window=20, negative=50, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e63a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_id = 2172\n",
    "# Ask the model for songs similar to song #2172\n",
    "model.wv.most_similar(positive=str(song_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190c1ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def print_recommendations(song_id):\n",
    "    similar_songs = np.array(\n",
    "    model.wv.most_similar(positive=str(song_id),topn=5)\n",
    "    )[:,0]\n",
    "    return songs_df.iloc[similar_songs]\n",
    "# Extract recommendations\n",
    "print_recommendations(2172)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
